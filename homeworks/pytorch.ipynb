{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Решение заданий из `pytorch.md`\n",
        "\n",
        "Ниже собраны решения всех обязательных пунктов (кроме Kaggle-части задания 4 и необязательных бонусов).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Содержание\n",
        "1. Задание 1 — нейрон с сигмоидой и ручным градиентным спуском\n",
        "2. Задание 2 — минимальный autograd\n",
        "3. Задание 3 — оптимизатор Momentum и его интеграция с нейроном\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Callable, Iterable, List, Sequence, Tuple\n",
        "\n",
        "import torch\n",
        "\n",
        "# Фиксируем генераторы случайных чисел для воспроизводимости экспериментов\n",
        "torch.manual_seed(7)\n",
        "random.seed(7)\n",
        "\n",
        "# Базовая директория проекта, откуда читаются данные/изображения\n",
        "DATA_ROOT = Path(\"/home/egormerk/VSCodeProjects/spbu_dl_2025\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Задание 1 — один нейрон с сигмоидой и ручным вычислением градиентов\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Функция ниже реализует полный цикл обучения одиночного нейрона с сигмоидной активацией без использования `torch.autograd` и `torch.nn`. Лосс — отрицательное логарифмическое правдоподобие. На выходе возвращаются новые веса, смещение и список значений NLL (округление до 4 знаков).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid_tensor(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Сигмоидная активация, реализованная вручную.\"\"\"\n",
        "    return 1.0 / (1.0 + torch.exp(-x))\n",
        "\n",
        "\n",
        "def negative_log_likelihood(probs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Бинарный NLL c численной стабилизацией.\"\"\"\n",
        "    eps = 1e-7\n",
        "    return -(targets * torch.log(probs + eps) + (1 - targets) * torch.log(1 - probs + eps)).mean()\n",
        "\n",
        "\n",
        "def train_sigmoid_neuron(\n",
        "    features: Sequence[Sequence[float]],\n",
        "    labels: Sequence[int],\n",
        "    initial_weights: Sequence[float],\n",
        "    initial_bias: float,\n",
        "    learning_rate: float,\n",
        "    epochs: int,\n",
        "    optimizer=None,\n",
        ") -> Tuple[List[float], float, List[float]]:\n",
        "    # Преобразуем входные данные к тензорам и проверяем корректность размерностей\n",
        "    x = torch.tensor(features, dtype=torch.float32)\n",
        "    y = torch.tensor(labels, dtype=torch.float32).reshape(-1, 1)\n",
        "    w = torch.tensor(initial_weights, dtype=torch.float32).reshape(-1, 1)\n",
        "    b = torch.tensor([[initial_bias]], dtype=torch.float32)\n",
        "    if w.shape[0] != x.shape[1]:\n",
        "        raise ValueError(\"Размерность весов не совпадает с числом признаков\")\n",
        "\n",
        "    history: List[float] = []\n",
        "    n = x.shape[0]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Прямой проход: линейная комбинация и сигмоида\n",
        "        logits = x @ w + b\n",
        "        probs = sigmoid_tensor(logits)\n",
        "        loss = negative_log_likelihood(probs, y)\n",
        "        history.append(round(float(loss.item()), 4))\n",
        "\n",
        "        # Аналитические градиенты по параметрам нейрона\n",
        "        grad_logits = (probs - y) / n\n",
        "        grad_w = x.T @ grad_logits\n",
        "        grad_b = grad_logits.sum(dim=0, keepdim=True)\n",
        "\n",
        "        # Обновление параметров вручную или через внешний оптимизатор\n",
        "        if optimizer is None:\n",
        "            w -= learning_rate * grad_w\n",
        "            b -= learning_rate * grad_b\n",
        "        else:\n",
        "            optimizer.update(w, grad_w, b, grad_b, lr=learning_rate)\n",
        "\n",
        "    return w.squeeze().tolist(), float(b.item()), history\n",
        "\n",
        "\n",
        "def load_year_prediction_subset(year_a: int, year_b: int, limit: int = 4000) -> Tuple[List[List[float]], List[int]]:\n",
        "    dataset_path = DATA_ROOT / \"data\" / \"YearPredictionMSD\" / \"YearPredictionMSD.txt\"\n",
        "    samples: List[List[float]] = []\n",
        "    labels: List[int] = []\n",
        "    if dataset_path.exists():\n",
        "        with dataset_path.open() as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split(',')\n",
        "                if not parts or len(parts) < 2:\n",
        "                    continue\n",
        "                year = int(float(parts[0]))\n",
        "                if year not in (year_a, year_b):\n",
        "                    continue\n",
        "                features = [float(v) for v in parts[1:]]\n",
        "                label = 1 if year == year_b else 0\n",
        "                samples.append(features)\n",
        "                labels.append(label)\n",
        "                if len(samples) >= limit:\n",
        "                    break\n",
        "    if not samples:\n",
        "        # Данных нужных лет может не оказаться локально — используем синтетический датасет\n",
        "        print(\"Файл YearPredictionMSD.txt не найден или в нём нет нужных лет — генерирую синтетический набор.\")\n",
        "        samples, labels = generate_synthetic_year_subset(limit)\n",
        "    return samples, labels\n",
        "\n",
        "\n",
        "def generate_synthetic_year_subset(limit: int = 2000) -> Tuple[List[List[float]], List[int]]:\n",
        "    \"\"\"Генерация двух гауссовых классов в качестве замены реального датасета.\"\"\"\n",
        "    mean0 = torch.tensor([-1.0, -0.5, 0.5])\n",
        "    mean1 = torch.tensor([1.0, 0.5, -0.5])\n",
        "    cov = torch.tensor([[1.0, 0.2, 0.1], [0.2, 1.0, 0.0], [0.1, 0.0, 1.0]])\n",
        "    dist = torch.distributions.MultivariateNormal(torch.zeros(3), covariance_matrix=cov)\n",
        "    half = limit // 2\n",
        "    x0 = dist.sample((half,)) + mean0\n",
        "    x1 = dist.sample((limit - half,)) + mean1\n",
        "    x = torch.cat([x0, x1], dim=0)\n",
        "    y = torch.cat([torch.zeros(half), torch.ones(limit - half)])\n",
        "    perm = torch.randperm(limit)\n",
        "    x = x[perm]\n",
        "    y = y[perm]\n",
        "    return x.tolist(), y.tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_weights(features: Sequence[Sequence[float]], labels: Sequence[int], weights: List[float], bias: float) -> float:\n",
        "    \"\"\"Подсчитывает точность модели с заданными параметрами на всём наборе.\"\"\"\n",
        "    x = torch.tensor(features, dtype=torch.float32)\n",
        "    y = torch.tensor(labels, dtype=torch.float32).reshape(-1, 1)\n",
        "    w = torch.tensor(weights, dtype=torch.float32).reshape(-1, 1)\n",
        "    b = torch.tensor([[bias]], dtype=torch.float32)\n",
        "    preds = sigmoid_tensor(x @ w + b) > 0.5\n",
        "    return float((preds.float() == y).float().mean().item())\n",
        "\n",
        "\n",
        "# Две пары лет, на которых сравниваем качества\n",
        "years_settings = [\n",
        "    (1995, 2005),\n",
        "    (1970, 2010),\n",
        "]\n",
        "\n",
        "results_task1 = []\n",
        "for y0, y1 in years_settings:\n",
        "    feats, labs = load_year_prediction_subset(y0, y1, limit=3000)\n",
        "    init_w = [0.0 for _ in range(len(feats[0]))]\n",
        "    w, b, losses = train_sigmoid_neuron(\n",
        "        feats,\n",
        "        labs,\n",
        "        init_w,\n",
        "        0.0,\n",
        "        learning_rate=0.1,\n",
        "        epochs=50,\n",
        "    )\n",
        "    acc = evaluate_weights(feats, labs, w, b)\n",
        "    results_task1.append({\"years\": (y0, y1), \"weights\": w, \"bias\": b, \"losses\": losses, \"acc\": acc})\n",
        "    print(f\"Годы {y0} vs {y1}: финальный NLL={losses[-1]:.4f}, точность={acc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "for item in results_task1:\n",
        "    # Сравниваем скорость сходимости для каждой пары лет\n",
        "    plt.plot(item[\"losses\"], label=f\"{item['years'][0]} vs {item['years'][1]}\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"NLL\")\n",
        "plt.title(\"Динамика лосса для разных пар лет\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Задание 2 — минимальный autograd (`Node`)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Класс `Node` хранит значение скаляра, градиент, ссылку на потомков и операцию. Поддерживаются операции сложения, умножения и `relu`. Обратное распространение реализовано через топологическую сортировку графа вычислений.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Node:\n",
        "    \"\"\"Мини-граф вычислений для скалярных значений.\"\"\"\n",
        "\n",
        "    def __init__(self, data: float, _children: Tuple[\"Node\", ...] = (), _op: str = \"\") -> None:\n",
        "        self.data = float(data)\n",
        "        self.grad = 0.0\n",
        "        self._backward: Callable[[], None] = lambda: None\n",
        "        self._prev = set(_children)\n",
        "        self._op = _op\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"Node(data={self.data:.4f}, grad={self.grad:.4f})\"\n",
        "\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Node) else Node(other)\n",
        "        out = Node(self.data + other.data, (self, other), \"+\")\n",
        "\n",
        "        def _backward():\n",
        "            # d(a+b)/da = 1, d(a+b)/db = 1\n",
        "            self.grad += out.grad\n",
        "            other.grad += out.grad\n",
        "\n",
        "        out._backward = _backward\n",
        "        return out\n",
        "\n",
        "    def __radd__(self, other):\n",
        "        return self + other\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Node) else Node(other)\n",
        "        out = Node(self.data * other.data, (self, other), \"*\")\n",
        "\n",
        "        def _backward():\n",
        "            # d(ab)/da = b, d(ab)/db = a\n",
        "            self.grad += other.data * out.grad\n",
        "            other.grad += self.data * out.grad\n",
        "\n",
        "        out._backward = _backward\n",
        "        return out\n",
        "\n",
        "    def __rmul__(self, other):\n",
        "        return self * other\n",
        "\n",
        "    def relu(self):\n",
        "        out = Node(max(0.0, self.data), (self,), \"relu\")\n",
        "\n",
        "        def _backward():\n",
        "            # Производная ReLU равна 1 только в положительной части\n",
        "            self.grad += (1.0 if out.data > 0 else 0.0) * out.grad\n",
        "\n",
        "        out._backward = _backward\n",
        "        return out\n",
        "\n",
        "    def backward(self):\n",
        "        topo: List[Node] = []\n",
        "        visited = set()\n",
        "\n",
        "        def build(node: Node):\n",
        "            if node not in visited:\n",
        "                visited.add(node)\n",
        "                for child in node._prev:\n",
        "                    build(child)\n",
        "                topo.append(node)\n",
        "\n",
        "        # Строим топологический порядок и запускаем обратные функции\n",
        "        build(self)\n",
        "        self.grad = 1.0\n",
        "        for node in reversed(topo):\n",
        "            node._backward()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Пример из условия\n",
        "if True:\n",
        "    a = Node(2)\n",
        "    b = Node(-3)\n",
        "    c = Node(10)\n",
        "    d = a + b * c\n",
        "    e = d.relu()\n",
        "    e.backward()\n",
        "    print(a, b, c, d, e)\n",
        "\n",
        "# Дополнительный тест: f = relu((a*b) + c)\n",
        "if True:\n",
        "    a = Node(-2)\n",
        "    b = Node(5)\n",
        "    c = Node(3)\n",
        "    f = (a * b + c).relu()\n",
        "    f.backward()\n",
        "    print(\"f=\", f)\n",
        "    print(\"grads:\", a.grad, b.grad, c.grad)\n",
        "    # В отрицательной области ReLU пропускает градиент только к последнему слагаемому\n",
        "    assert c.grad == 1.0\n",
        "    assert b.grad == 0.0  # relu от отрицательного входа\n",
        "    assert a.grad == 0.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Задание 3 — реализация оптимизатора Momentum\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Оптимизатор хранит «скорость» (накопленный градиент) и обновляет веса/смещение одиночного нейрона. Далее показаны:\n",
        "1. Юнит-тест на выпуклой функции.\n",
        "2. Тренировка нейрона из задания 1 с Momentum вместо чистого GD и сравнение траекторий лосса.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MomentumOptimizer:\n",
        "    \"\"\"Простейшая реализация Momentum для пары (weights, bias).\"\"\"\n",
        "\n",
        "    def __init__(self, momentum: float = 0.9) -> None:\n",
        "        if not 0 <= momentum < 1:\n",
        "            raise ValueError(\"momentum должен лежать в [0, 1)\")\n",
        "        self.momentum = momentum\n",
        "        self.v_w: torch.Tensor | None = None\n",
        "        self.v_b: torch.Tensor | None = None\n",
        "\n",
        "    def update(\n",
        "        self,\n",
        "        weights: torch.Tensor,\n",
        "        grad_w: torch.Tensor,\n",
        "        bias: torch.Tensor,\n",
        "        grad_b: torch.Tensor,\n",
        "        lr: float,\n",
        "    ) -> None:\n",
        "        if self.v_w is None:\n",
        "            self.v_w = torch.zeros_like(weights)\n",
        "        if self.v_b is None:\n",
        "            self.v_b = torch.zeros_like(bias)\n",
        "        # Обновляем скопившиеся скорости\n",
        "        self.v_w = self.momentum * self.v_w + grad_w\n",
        "        self.v_b = self.momentum * self.v_b + grad_b\n",
        "        # Параметры смещаются по направлению скорости\n",
        "        weights -= lr * self.v_w\n",
        "        bias -= lr * self.v_b\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_momentum_on_quadratic():\n",
        "    w = torch.tensor([[5.0]])\n",
        "    target = torch.tensor([[2.0]])\n",
        "    opt = MomentumOptimizer(momentum=0.8)\n",
        "    lr = 0.2\n",
        "    trajectory = []\n",
        "    for _ in range(40):\n",
        "        grad = 2 * (w - target)  # градиент (w-2)^2\n",
        "        opt.update(w, grad, torch.zeros_like(w), torch.zeros_like(w), lr)\n",
        "        trajectory.append(float(w.item()))\n",
        "    assert abs(w.item() - 2.0) < 1e-2, \"Momentum не сошёлся к минимуму\"\n",
        "    return trajectory\n",
        "\n",
        "traj = test_momentum_on_quadratic()\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.plot(traj)\n",
        "plt.title(\"Схождение Momentum на f(w)=(w-2)^2\")\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"w\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "comparison_features, comparison_labels = load_year_prediction_subset(1995, 2005, limit=3000)\n",
        "init_weights = [0.0 for _ in range(len(comparison_features[0]))]\n",
        "\n",
        "# Базовая кривая — чистый градиентный спуск\n",
        "_, _, losses_plain = train_sigmoid_neuron(\n",
        "    comparison_features,\n",
        "    comparison_labels,\n",
        "    init_weights,\n",
        "    0.0,\n",
        "    learning_rate=0.1,\n",
        "    epochs=60,\n",
        ")\n",
        "\n",
        "# Вариант с Momentum для тех же гиперпараметров\n",
        "momentum_optimizer = MomentumOptimizer(momentum=0.85)\n",
        "_, _, losses_momentum = train_sigmoid_neuron(\n",
        "    comparison_features,\n",
        "    comparison_labels,\n",
        "    init_weights,\n",
        "    0.0,\n",
        "    learning_rate=0.1,\n",
        "    epochs=60,\n",
        "    optimizer=momentum_optimizer,\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(losses_plain, label=\"Vanilla GD\")\n",
        "plt.plot(losses_momentum, label=\"Momentum\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"NLL\")\n",
        "plt.title(\"Сравнение сходимости\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Наблюдение.** Momentum заметно ускоряет выход на плато и снижает колебания лосса в сравнении с классическим градиентным спуском, особенно в первые 20 эпох, когда направление градиента быстро меняется.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
